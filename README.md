# Adapting-LLMs-to-Native-Multimodal-Models
The structure of large language models (LLMs) is typically: text -> text encoder -> Transformer -> text decoder -> text. Native multimodality can be achieved by adding audio encoder and decoder modules and retraining the model.
